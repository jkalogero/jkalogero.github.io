<!DOCTYPE html>
<html lang="en">

<title>Scale Equivariant Graph Metanetworks</title>

<head>

    <meta content="Hugo 0.113.0" name="generator">

    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">


    <link href="https://fonts.googleapis.com" rel="preconnect">
    <link crossorigin href="https://fonts.gstatic.com" rel="preconnect">
    <link href="https://fonts.googleapis.com/css2?family=Alegreya:wght@500&amp;family=Alegreya+SC&amp;family=Lora:wght@400&amp;display=swap"
          rel="stylesheet">


    <link crossorigin="anonymous" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css"
          integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" rel="stylesheet">


    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

    <link href="/projects/css-style.css" rel="stylesheet">

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet">
    <!--    <script crossorigin="anonymous"-->
    <!--            integrity="sha384-IQsoLXl5PILFhosVNubq5LC7Qb9DXgDA9i+tQ8Zj3iwWAwPtgFTxbJ8NT4GN1R8p"-->
    <!--            src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.2/dist/umd/popper.min.js"></script>-->
    <link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" rel="stylesheet">
    <script crossorigin="anonymous"
            integrity="sha384-Atwg2Pkwv9vp0ygtn1JAojH0nYbwNJLPhwyoVbhoPwBhjQPR5VtM2+xf0Uwh9KtT"
            src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.min.js"></script>

    <script src="/scripts.js">
    </script>
    <script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
            type="text/javascript">
    </script>


    <script>
        loadFooter();
    </script>
</head>
<body>
<div id="outer">
    <div class="card col-11 mx-auto" id="main">
        <div class="title">
            Scale Equivariant Graph Metanetworks
        </div>
        <div class="authors">
            <a href="http://jkalogero.github.io" target="_blank">Ioannis Kalogeropoulos</a><sup>1,2</sup>*,
            <a href="http://users.uoa.gr/~gbouritsas/" target="_blank">Giorgos Bouritsas</a><sup>1,2</sup>*,
            <a href="http://users.uoa.gr/~yannisp/" target="_blank">Yannis Panagakis</a><sup>1,2</sup>
        </div>
        <div class="institutions">
            <sup>1</sup>National and Kapodistrian University of Athens, <sup>2</sup>Archimedes/Athena RC, Greece
        </div>
        <div class="venue-name">NeurIPS 2024 (Oral)</div>
        <div class="links">
            <span class="link-container">
                <a class="ext-links" href="https://neurips.cc/virtual/2024/oral/97993">
                  <span class="icon">
                      <img alt="NeurIPS Logo" src="https://neurips.cc/static/core/img/NIPS-logo.svg"
                           style="width: 45px">
                  </span>
                  <span>NeurIPS 2024</span>
                </a>
              </span>
            <span class="link-container">
                <a class="ext-links" href="https://arxiv.org/pdf/2406.10685">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            <span class="link-container">
                <a class="ext-links" href="https://github.com/jkalogero/scalegmn">
                  <span class="icon">
                      <i class="fa fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            <span class="link-container">
                <a class="ext-links" href="poster.pdf">
                  <span class="icon">
                      <i class="fa fa-file"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
            <span class="link-container">
                <a class="ext-links" href="slides.pdf">
                  <span class="icon">
                      <i class="fa fa-film"></i>
                  </span>
                  <span>Slides</span>
                </a>
              </span>

        </div>
        <div class="col-sm-8 container">
            <!--  Add new section      -->
            <div class="container-abstract">
                <div class="section-title">
                    <h2>Abstract</h2>
                </div>
                <p>
                    This paper pertains to an emerging machine learning paradigm: learning higher-order functions,
                    i.e. functions whose inputs are functions themselves, <i>particularly
                    when these inputs are Neural Networks (NNs)</i>. With the growing interest in architectures
                    that process NNs, a recurring design principle has permeated the field:
                    adhering to the permutation symmetries arising from the connectionist structure of
                    NNs. <i>However, are these the sole symmetries present in NN parameterizations?</i>
                    Zooming into most practical activation functions (e.g. sine, ReLU, tanh) answers
                    this question negatively and gives rise to intriguing new symmetries, which we
                    collectively refer to as <i>scaling symmetries</i>, that is, non-zero scalar multiplications
                    and divisions of weights and biases. In this work, we propose Scale Equivariant
                    Graph MetaNetworks - ScaleGMNs, a framework that adapts the Graph Metanetwork (message-passing)
                    paradigm
                    by incorporating scaling symmetries and thus rendering neuron and edge representations equivariant
                    to valid scalings. We introduce novel building
                    blocks, of independent technical interest, that allow for equivariance or invariance with respect to
                    individual scalar multipliers or their
                    product and use them in all components of ScaleGMN. Furthermore, we prove
                    that, under certain expressivity conditions, ScaleGMN can simulate the forward
                    and backward pass of any input feedforward neural network. Experimental results
                    demonstrate that our method advances the state-of-the-art performance for several
                    datasets and activation functions, highlighting the power of scaling symmetries as
                    an inductive bias for NN processing.
                </p>

                <div class="section-title">
                    <h2>Overview</h2>
                </div>

                <p>Directly operating on the parameters of neural networks, paves the way to a whole new spectrum of
                    exciting applications.
                    From analysis and intepretation of trained NNs to editing or even generating neural networks.
                    Illustratively, we could apply domain adaptation to trained models by directly
                    operating on their parameters
                    or have models that outperform classic optimization algorithms in a learned optimization setting.
                    Furthermore, with the advent of Implicit Neural Representations, trained NN parameters are
                    increasingly used to
                    represent datapoint signals, such as images or 3D shapes, replacing raw
                    representations, i.e., pixel grids or point clouds. Consequently, many tasks involving such
                    data, across various domains such as computer vision and physics, which are currently tackled
                    using domain-specific architectures (e.g. CNNs for grids, PointNets or GNNs for point
                    clouds and meshes), could potentially be solved by <i>NNs that process the parameters of other
                        NNs.</i></p>
                <div class="full-width">Neural networks, however, have symmetries, i.e. there exist a few
                    transformations
                    that when applied to the parameters of the model leave the underlying function intact:
                    <ul>
                        <li><strong>Permutation</strong> symmetries: Hidden neurons do not possess any inherent
                            ordering.
                        </li>
                        <li><strong>Scaling</strong> symmetries: Activation functions have inherent symmetries bestowed
                            to the NN.
                        </li>
                    </ul>
                </div>

                <div class="row gif-collection">
                    <div class="col-4 gif-container">
                        <div class="fig-title"><strong>Permutation</strong></div>
                        <img class="img-fluid center-block sym-gif" src="figs/perm_sym.gif">
                    </div>
                    <div class="col-4 gif-container">
                        <div class="fig-title"><strong>Scaling</strong></div>
                        <img class="img-fluid center-block sym-gif" src="figs/scale_sym.gif">
                    </div>
                    <div class="col-4 gif-container">
                        <div class="fig-title"><strong>Both</strong></div>
                        <img class="img-fluid center-block sym-gif" src="figs/pq_sym.gif">
                    </div>
                </div>
                <p>In this work we propose a new type of metanetwork, called <strong>ScaleGMN</strong>, which
                    accounts for both permutation and scaling symmetries.
                    After mapping the input NN to a graph and applying a proper initialization method, we feed the
                    result to a Graph Neural Network.
                    Since GNN layers are by construction permutation equivariant, we only need to <i>extend the MPNN
                        paradigm to account for the scaling symmetries</i>. For this scope we design <strong>scale
                        equivariant MSG and
                        UPD functions</strong> and a <strong>permutation and scale invariant READOUT function</strong>.
                </p>
                <div class="section-title">
                    <h2>Paper</h2>
                </div>
                <div class="thumbnail-fig">
                    <a href="https://arxiv.org/pdf/2406.10685v2">
                        <img class="img-fluid center-block sym-gif" src="figs/paper_thumbnail.png">
                    </a>
                </div>

                <div class="section-title">
                    <h2>Bibtex</h2>
                </div>
                <div class="publication-bibtext col-8 code-area">
                        <pre class="pup_bibtext">@article{kalogeropoulos2024scale,
title={Scale Equivariant Graph Metanetworks},
author={Kalogeropoulos, Ioannis and Bouritsas, Giorgos and Panagakis, Yannis},
journal={Advances in neural information processing systems (NeurIPS)},
year={2024}
}</pre>
                    <a class="btn-sm badge-button" href="javascript:void(0)"
                       onclick="copyBibtex(this)">Copy</a>

                </div>
            </div>
        </div>
    </div>
    <div id="footer"></div>
</div>

<!--    <script>-->
<!--        // Call loadNavbar() after the script is loaded-->
<!--        window.addEventListener('DOMContentLoaded', (event) => {-->
<!--            loadNavbar();-->
<!--        });-->
<!--    </script>-->

</body>

</html>
